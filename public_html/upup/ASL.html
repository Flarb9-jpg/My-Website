<!DOCTYPE html>
<html>
    <head>
		<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2MGCXY8XKD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2MGCXY8XKD');
</script>
  <meta charset="UTF-8" /><script type="text/javascript">(window.NREUM||(NREUM={})).loader_config={licenseKey:"e7fb1b89a0",applicationID:"750147145"};window.NREUM||(NREUM={}),__nr_require=function(t,e,n){function r(n){if(!e[n]){var i=e[n]={exports:{}};t[n][0].call(i.exports,function(e){var i=t[n][1][e];return r(i||e)},i,i.exports)}return e[n].exports}if("function"==typeof __nr_require)return __nr_require;for(var i=0;i<n.length;i++)r(n[i]);return r}({1:[function(t,e,n){function r(){}function i(t,e,n){return function(){return o(t,[u.now()].concat(f(arguments)),e?null:this,n),e?void 0:this}}var o=t("handle"),a=t(8),f=t(9),c=t("ee").get("tracer"),u=t("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var d=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],p="api-",l=p+"ixn-";a(d,function(t,e){s[e]=i(p+e,!0,"api")}),s.addPageAction=i(p+"addPageAction",!0),s.setCurrentRouteName=i(p+"routeName",!0),e.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(t,e){var n={},r=this,i="function"==typeof e;return o(l+"tracer",[u.now(),t,n],r),function(){if(c.emit((i?"":"no-")+"fn-start",[u.now(),r,i],n),i)try{return e.apply(this,arguments)}catch(t){throw c.emit("fn-err",[arguments,this,t],n),t}finally{c.emit("fn-end",[u.now()],n)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(t,e){m[e]=i(l+e)}),newrelic.noticeError=function(t,e){"string"==typeof t&&(t=new Error(t)),o("err",[t,u.now(),!1,e])}},{}],2:[function(t,e,n){function r(t){if(NREUM.init){for(var e=NREUM.init,n=t.split("."),r=0;r<n.length-1;r++)if(e=e[n[r]],"object"!=typeof e)return;return e=e[n[n.length-1]]}}e.exports={getConfiguration:r}},{}],3:[function(t,e,n){function r(){return f.exists&&performance.now?Math.round(performance.now()):(o=Math.max((new Date).getTime(),o))-a}function i(){return o}var o=(new Date).getTime(),a=o,f=t(10);e.exports=r,e.exports.offset=a,e.exports.getLastTimestamp=i},{}],4:[function(t,e,n){function r(t){return!(!t||!t.protocol||"file:"===t.protocol)}e.exports=r},{}],5:[function(t,e,n){function r(t,e){var n=t.getEntries();n.forEach(function(t){"first-paint"===t.name?d("timing",["fp",Math.floor(t.startTime)]):"first-contentful-paint"===t.name&&d("timing",["fcp",Math.floor(t.startTime)])})}function i(t,e){var n=t.getEntries();n.length>0&&d("lcp",[n[n.length-1]])}function o(t){t.getEntries().forEach(function(t){t.hadRecentInput||d("cls",[t])})}function a(t){if(t instanceof m&&!g){var e=Math.round(t.timeStamp),n={type:t.type};e<=p.now()?n.fid=p.now()-e:e>p.offset&&e<=Date.now()?(e-=p.offset,n.fid=p.now()-e):e=p.now(),g=!0,d("timing",["fi",e,n])}}function f(t){d("pageHide",[p.now(),t])}if(!("init"in NREUM&&"page_view_timing"in NREUM.init&&"enabled"in NREUM.init.page_view_timing&&NREUM.init.page_view_timing.enabled===!1)){var c,u,s,d=t("handle"),p=t("loader"),l=t(7),m=NREUM.o.EV;if("PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){c=new PerformanceObserver(r);try{c.observe({entryTypes:["paint"]})}catch(v){}u=new PerformanceObserver(i);try{u.observe({entryTypes:["largest-contentful-paint"]})}catch(v){}s=new PerformanceObserver(o);try{s.observe({type:"layout-shift",buffered:!0})}catch(v){}}if("addEventListener"in document){var g=!1,h=["click","keydown","mousedown","pointerdown","touchstart"];h.forEach(function(t){document.addEventListener(t,a,!1)})}l(f)}},{}],6:[function(t,e,n){function r(t,e){if(!i)return!1;if(t!==i)return!1;if(!e)return!0;if(!o)return!1;for(var n=o.split("."),r=e.split("."),a=0;a<r.length;a++)if(r[a]!==n[a])return!1;return!0}var i=null,o=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var f=navigator.userAgent,c=f.match(a);c&&f.indexOf("Chrome")===-1&&f.indexOf("Chromium")===-1&&(i="Safari",o=c[1])}e.exports={agent:i,version:o,match:r}},{}],7:[function(t,e,n){function r(t){function e(){t(a&&document[a]?document[a]:document[i]?"hidden":"visible")}"addEventListener"in document&&o&&document.addEventListener(o,e,!1)}e.exports=r;var i,o,a;"undefined"!=typeof document.hidden?(i="hidden",o="visibilitychange",a="visibilityState"):"undefined"!=typeof document.msHidden?(i="msHidden",o="msvisibilitychange"):"undefined"!=typeof document.webkitHidden&&(i="webkitHidden",o="webkitvisibilitychange",a="webkitVisibilityState")},{}],8:[function(t,e,n){function r(t,e){var n=[],r="",o=0;for(r in t)i.call(t,r)&&(n[o]=e(r,t[r]),o+=1);return n}var i=Object.prototype.hasOwnProperty;e.exports=r},{}],9:[function(t,e,n){function r(t,e,n){e||(e=0),"undefined"==typeof n&&(n=t?t.length:0);for(var r=-1,i=n-e||0,o=Array(i<0?0:i);++r<i;)o[r]=t[e+r];return o}e.exports=r},{}],10:[function(t,e,n){e.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(t,e,n){function r(){}function i(t){function e(t){return t&&t instanceof r?t:t?u(t,c,a):a()}function n(n,r,i,o,a){if(a!==!1&&(a=!0),!l.aborted||o){t&&a&&t(n,r,i);for(var f=e(i),c=v(n),u=c.length,s=0;s<u;s++)c[s].apply(f,r);var p=d[w[n]];return p&&p.push([b,n,r,f]),f}}function o(t,e){y[t]=v(t).concat(e)}function m(t,e){var n=y[t];if(n)for(var r=0;r<n.length;r++)n[r]===e&&n.splice(r,1)}function v(t){return y[t]||[]}function g(t){return p[t]=p[t]||i(n)}function h(t,e){l.aborted||s(t,function(t,n){e=e||"feature",w[n]=e,e in d||(d[e]=[])})}var y={},w={},b={on:o,addEventListener:o,removeEventListener:m,emit:n,get:g,listeners:v,context:e,buffer:h,abort:f,aborted:!1};return b}function o(t){return u(t,c,a)}function a(){return new r}function f(){(d.api||d.feature)&&(l.aborted=!0,d=l.backlog={})}var c="nr@context",u=t("gos"),s=t(8),d={},p={},l=e.exports=i();e.exports.getOrSetContext=o,l.backlog=d},{}],gos:[function(t,e,n){function r(t,e,n){if(i.call(t,e))return t[e];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,e,{value:r,writable:!0,enumerable:!1}),r}catch(o){}return t[e]=r,r}var i=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(t,e,n){function r(t,e,n,r){i.buffer([t],r),i.emit(t,e,n)}var i=t("ee").get("handle");e.exports=r,r.ee=i},{}],id:[function(t,e,n){function r(t){var e=typeof t;return!t||"object"!==e&&"function"!==e?-1:t===window?0:a(t,o,function(){return i++})}var i=1,o="nr@id",a=t("gos");e.exports=r},{}],loader:[function(t,e,n){function r(){if(!R++){var t=M.info=NREUM.info,e=v.getElementsByTagName("script")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&e))return u.abort();c(E,function(e,n){t[e]||(t[e]=n)});var n=a();f("mark",["onload",n+M.offset],null,"api"),f("timing",["load",n]);var r=v.createElement("script");0===t.agent.indexOf("http://")||0===t.agent.indexOf("https://")?r.src=t.agent:r.src=l+"://"+t.agent,e.parentNode.insertBefore(r,e)}}function i(){"complete"===v.readyState&&o()}function o(){f("mark",["domContent",a()+M.offset],null,"api")}var a=t(3),f=t("handle"),c=t(8),u=t("ee"),s=t(6),d=t(4),p=t(2),l=p.getConfiguration("ssl")===!1?"http":"https",m=window,v=m.document,g="addEventListener",h="attachEvent",y=m.XMLHttpRequest,w=y&&y.prototype,b=!d(m.location);NREUM.o={ST:setTimeout,SI:m.setImmediate,CT:clearTimeout,XHR:y,REQ:m.Request,EV:m.Event,PR:m.Promise,MO:m.MutationObserver};var x=""+location,E={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1209.min.js"},O=y&&w&&w[g]&&!/CriOS/.test(navigator.userAgent),M=e.exports={offset:a.getLastTimestamp(),now:a,origin:x,features:{},xhrWrappable:O,userAgent:s,disabled:b};if(!b){t(1),t(5),v[g]?(v[g]("DOMContentLoaded",o,!1),m[g]("load",r,!1)):(v[h]("onreadystatechange",i),m[h]("onload",r)),f("mark",["firstbyte",a.getLastTimestamp()],null,"api");var R=0}},{}],"wrap-function":[function(t,e,n){function r(t,e){function n(e,n,r,c,u){function nrWrapper(){var o,a,s,p;try{a=this,o=d(arguments),s="function"==typeof r?r(o,a):r||{}}catch(l){i([l,"",[o,a,c],s],t)}f(n+"start",[o,a,c],s,u);try{return p=e.apply(a,o)}catch(m){throw f(n+"err",[o,a,m],s,u),m}finally{f(n+"end",[o,a,p],s,u)}}return a(e)?e:(n||(n=""),nrWrapper[p]=e,o(e,nrWrapper,t),nrWrapper)}function r(t,e,r,i,o){r||(r="");var f,c,u,s="-"===r.charAt(0);for(u=0;u<e.length;u++)c=e[u],f=t[c],a(f)||(t[c]=n(f,s?c+r:r,i,c,o))}function f(n,r,o,a){if(!m||e){var f=m;m=!0;try{t.emit(n,r,o,e,a)}catch(c){i([c,n,r,o],t)}m=f}}return t||(t=s),n.inPlace=r,n.flag=p,n}function i(t,e){e||(e=s);try{e.emit("internal-error",t)}catch(n){}}function o(t,e,n){if(Object.defineProperty&&Object.keys)try{var r=Object.keys(t);return r.forEach(function(n){Object.defineProperty(e,n,{get:function(){return t[n]},set:function(e){return t[n]=e,e}})}),e}catch(o){i([o],n)}for(var a in t)l.call(t,a)&&(e[a]=t[a]);return e}function a(t){return!(t&&t instanceof Function&&t.apply&&!t[p])}function f(t,e){var n=e(t);return n[p]=t,o(t,n,s),n}function c(t,e,n){var r=t[e];t[e]=f(r,n)}function u(){for(var t=arguments.length,e=new Array(t),n=0;n<t;++n)e[n]=arguments[n];return e}var s=t("ee"),d=t(9),p="nr@original",l=Object.prototype.hasOwnProperty,m=!1;e.exports=r,e.exports.wrapFunction=f,e.exports.wrapInPlace=c,e.exports.argsToArray=u},{}]},{},["loader"]);</script>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
      <meta name="keywords"  content="Pierson Inc.,Google,Street,View,Trusted,Photographer,YouTube,George,McKinley,UPUP,Resume,Portfolio,Adobe,Premiere,Pro,After,Effects,UMiami,Ross,Pierson,Seabreeze High" />
      <meta name="description"  content="Welcome to my portfolio. Here you'll find all that you need to know." />
      <meta name="twitter:card"  content="summary_large_image" />
      <meta name="twitter:site"  content="@AdobePortfolio" />
      <meta  property="og:title" content="Ross Pierson - Research" />
      <meta  property="og:description" content="Welcome to my portfolio. Here you'll find all that you need to know." />
      <meta  property="og:image" content="https://pro2-bar-s3-cdn-cf5.myportfolio.com/0491e5eef528f0b73ffb52f38fd2dd09/64219fa1-22ee-4462-b411-2191f250bd73_rwc_0x59x532x416x532.JPG?h=dbf336b0730562fa26a3aa84c4b5309e" />
      <meta name="description" content="Welcome to my Portfolio!">
      <meta name="keywords" content="Portfolio">
      <meta name="rating" content="general">
      <meta name="copyright" content="Copyright ©2019 -">
      <meta name="expires" content="never">
      <meta name="distribution" content="local">
      <meta name="robots" content="index,follow">
        <link rel="icon" href="https://pro2-bar-s3-cdn-cf2.myportfolio.com/0491e5eef528f0b73ffb52f38fd2dd09/37af8b87-94d6-4e34-a3ad-04e3b3e82a46_carw_1x1x32.JPG?h=b3444a2de5050cb5583e26bc3a251f12" />
        <link rel="apple-touch-icon" href="https://pro2-bar-s3-cdn-cf.myportfolio.com/0491e5eef528f0b73ffb52f38fd2dd09/88480797-03a9-4452-a658-db297cd0d45e_carw_1x1x180.JPG?h=bd1668cca5410744b36ef40959c9482a" />
      <link rel="stylesheet" href="/dist/css/main.css" type="text/css" />
      <link rel="stylesheet" href="https://pro2-bar-s3-cdn-cf4.myportfolio.com/0491e5eef528f0b73ffb52f38fd2dd09/f06f5253028de93b17d8affbced89f6e1622073843.css?h=fdf0193096775ff854eb66096c3c9545" type="text/css" />
    <link rel="canonical" href="https://rosspierson.myportfolio.com/upup" />
      <title>Ross Pierson - Research</title>
    <script type="text/javascript" src="//use.typekit.net/ik/W3I-TODpbG90kZvQYRCpNAvIKS7YN3snt2xCCoFCoN3felbgfHYEBsJzwD9oFDIDWh4KwQ9Dw2MU5Qbaw2IoFRqUFcsRweZcjQjhF288FhwDZcwt5eJDjh9-mkG0dW83da4XZcNC-Av0jhNlOfG0SY4zwKuh-AmaOcuoSeNkieZzde8zOcFzdPUlpWgzS1scdhUTdkoRdhXCSY4zwKuh-AmaOcuoSeNkieZzde8zOcFzdPJIZhuD-KJIicFDdPJIpW8oSkG4fFIVIMMjgkMgH6qJn3IbMs6IJMI7fbKlMsMgeMj6MPGHfOWdMsMfeMb6MZMgr_P1a39.js?cb=" async onload="
    try {
      window.Typekit.load();
    } catch (e) {
      console.warn('Typekit not loaded.');
    }
    "></script>
</head>
  <body>  <div class="page-background-video page-background-video-with-panel">
  </div>
  <div class="js-responsive-nav">
    <div class="responsive-nav has-social">
      <div class="close-responsive-click-area js-close-responsive-nav">
        <div class="close-responsive-button"></div>
      </div>
          <div class="nav-container">
            <nav data-hover-hint="nav">
      <div class="page-title">
        <a href="/contact" >Contact</a>
      </div>
                <div class="gallery-title"><a href="/work" >Work</a></div>
                <div class="social pf-nav-social" data-hover-hint="navSocialIcons">
                  <ul>
                          <li>
                            <a href="https://www.youtube.com/channel/UCB2Bdlb8efRC0TqsGiepkig" target="_blank">
                              <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="Layer_1" viewBox="0 0 30 24" xml:space="preserve" class="icon"><path d="M26.15 16.54c0 0-0.22 1.57-0.9 2.26c-0.87 0.91-1.84 0.91-2.28 0.96C19.78 20 15 20 15 20 s-5.91-0.05-7.74-0.23c-0.51-0.09-1.64-0.07-2.51-0.97c-0.68-0.69-0.91-2.26-0.91-2.26s-0.23-1.84-0.23-3.68v-1.73 c0-1.84 0.23-3.68 0.23-3.68s0.22-1.57 0.91-2.26c0.87-0.91 1.83-0.91 2.28-0.96C10.22 4 15 4 15 4H15c0 0 4.8 0 8 0.2 c0.44 0.1 1.4 0.1 2.3 0.96c0.68 0.7 0.9 2.3 0.9 2.26s0.23 1.8 0.2 3.68v1.73C26.38 14.7 26.1 16.5 26.1 16.54z M12.65 8.56l0 6.39l6.15-3.18L12.65 8.56z"/></svg>
                            </a>
                          </li>
                  </ul>
                </div>
            </nav>
          </div>
    </div>
  </div>
  <div class="site-wrap cfix js-site-wrap">
    <div class="site-container">
      <div class="site-content e2e-site-content">
        <header class="site-header">
          <div class="logo-container">
              <div class="logo-wrap" data-hover-hint="logo">
                    <div class="logo e2e-site-logo-text logo-text  ">
    <a href="/work" class="preserve-whitespace">Ross Pierson</a>

</div>
              </div>
  <div class="hamburger-click-area js-hamburger">
    <div class="hamburger">
      <i></i>
      <i></i>
      <i></i>
    </div>
  </div>
          </div>
              <div class="nav-container">
                <nav data-hover-hint="nav">
      <div class="page-title">
        <a href="/contact" >Contact</a>
      </div>
                <div class="gallery-title"><a href="/work" >Work</a></div>
                    <div class="social pf-nav-social" data-hover-hint="navSocialIcons">
                      <ul>
                              <li>
                                <a href="https://www.youtube.com/channel/UCB2Bdlb8efRC0TqsGiepkig" target="_blank">
                                  <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="Layer_1" viewBox="0 0 30 24" xml:space="preserve" class="icon"><path d="M26.15 16.54c0 0-0.22 1.57-0.9 2.26c-0.87 0.91-1.84 0.91-2.28 0.96C19.78 20 15 20 15 20 s-5.91-0.05-7.74-0.23c-0.51-0.09-1.64-0.07-2.51-0.97c-0.68-0.69-0.91-2.26-0.91-2.26s-0.23-1.84-0.23-3.68v-1.73 c0-1.84 0.23-3.68 0.23-3.68s0.22-1.57 0.91-2.26c0.87-0.91 1.83-0.91 2.28-0.96C10.22 4 15 4 15 4H15c0 0 4.8 0 8 0.2 c0.44 0.1 1.4 0.1 2.3 0.96c0.68 0.7 0.9 2.3 0.9 2.26s0.23 1.8 0.2 3.68v1.73C26.38 14.7 26.1 16.5 26.1 16.54z M12.65 8.56l0 6.39l6.15-3.18L12.65 8.56z"/></svg>
                                </a>
                              </li>
                      </ul>
                    </div>
                </nav>
              </div>
        </header>
        <main>
  <div class="page-container" data-context="page.page.container" data-hover-hint="pageContainer">
    <section class="page standard-modules">
        <header class="page-header content" data-context="pages" data-identity="id:p5be8f7798f11640a95b8e7f2e9b66fcd1021ec5488d61ecbdb1d3" data-hover-hint="pageHeader" data-hover-hint-id="p5be8f7798f11640a95b8e7f2e9b66fcd1021ec5488d61ecbdb1d3">
            <h1 class="title preserve-whitespace e2e-site-logo-text">American Sign Language to English Text</h1>
            <p class="description"></p>
        </header>
        </section>
        </div>
    <div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
  <li><a href="#First_Point_Header">1 Purpose</a></li>
  <li><a href="#Second_Point_Header">2 Overview</a></li>
  <li><a href="#Third_Point_Header">3 Problem Analysis</a></li>
  <li><a href="#Fourth_Point_Header">4 Background</a></li>
  <li><a href="#Fifth_Point_Header">5 Planning</a>
  <ul>
    <li><a href="#Fifth_Sub_Point_1">5.1 Software and Hardware Design</a></li>
    <li><a href="#Fifth_Sub_Point_2">5.2 Algorithm and Data Structure Selection</a></li>
  </ul>
  </li>
  <li><a href="#Sixth_Point_Header">6 User Interface Design</a>
  <ul>
    <li><a href="#Sixth_Sub_Point_1">6.1 Storyboard</a></li>
    <li><a href="#Sixth_Sub_Point_2">6.2 UML Diagram</a></li>
    <li><a href="#Sixth_Sub_Point_3">6.3 User Interface Flowchart</a></li>
  </ul>
  </li>
  <li><a href="#Seventh_Point_Header">7 Verification and Validation</a></li>
  <li><a href="#Eigth_Point_Header">8 Prototyping</a></li>
  <li><a href="#Ninth_Point_Header">9 Conclusion</a></li>
  <li><a href="#Tenth_Point_Header">10 References</a></li>
</ul>
</div>
<div class="page-content js-page-content" data-context="pages" data-identity="id:p5be8f7798f11640a95b8e7f2e9b66fcd1021ec5488d61ecbdb1d3">
        <div id="project-canvas" class="js-project-modules modules content">
          <div id="project-modules"><div class="project-module module text project-module-text align- js-project-module e2e-site-project-module-text">
  <div class="rich-text js-text-editable module-text">			
<h2 id="First_Point_Header" style="text-align:center">1 Purpose</h2>
<p > For the Computer Science 410 class, my project idea is to create an application for the American Sign Language community by translating the ASL for the non-signing community.  This idea was inspired by the premise of Google Translate and its ability to translate certain languages using a camera in real time. Since mostly everyone with a smartphone knows how to text, I will primarily focus on translating the visual language of ASL to printed word where the app will record what is being signed and translate it into text for the non-signer to understand the gestures.  My ultimate hope is that this app will give the ASL community another avenue of communication and vise-a-versa, in such scenarios and emergencies where immediately being able to interpret what is being signed is beneficial versus standing there passing text.
In order to complete my goal and to be in conjunction with my class requirements, I plan on working approximately 50 hours per credit. In total, this will add up to be three credits and 150 hours for the entire semester.  With the approximation of about 13 weeks per semester, this divides out to be 11.5 hours a week.  With my current schedule of classes I am taking for the Spring semester, I will be able to accomplish this task by working on average for 1.5 to two hours a day. With all my classes ending around 3:00 pm, this should not be a problem. Furthermore, with Professor Ogihara as my project supervisor, he will be able to provide me guidance in any situation where the project may extend outside my knowledge. By the end of the semester, I hope to have a complete pseudocode outline of the entire project with supporting documents that will be able to be translated and transferred directly into a working prototype so that it could be worked on next semester.</p></div></div></div></div></div>
<h2 id="Second_Point_Header" style="text-align:center">2 Overview</h2>
<p>The app overview will consist of using the camera of a smartphone to record the movements of the ASL user, interpret these actions, and then produce a worded paragraph translation of what they said.  Throughout the course of this semester, I will mostly be planning the coding necessary to accurately translate the first stage basics of American Sign Language.  To begin with, I plan on using Python’s Machine Learning capabilities or JavaScript’s Web Speech API algorithms to track the movements of the user’s hands and to interpret what each signal means.  Since ASL can sometimes include complex hand movements to convey an idea, I will start off with fingerspelling analysis that doesn’t require any hand movement and is more interpretable as to when one letter is spelled out and moving on to the next letter. Since this app is designed to be mobile with smartphone usage, I will first start incorporating the project into Android Studio and then into Swift for Apple devices if time permits.  In order to realize the scope of my goal for this project, I plan to create a variety of supporting documents, such as a UML diagram and storyboard to help lay out the app’s usability.  I believe the UML diagram will be of the greatest assistance to me by visually seeing how complex the app may turn out, and how in-depth, including the time commitment required, it will be to walk through each step of the app creation for its initial phase.
</p>
<h2 id="Third_Point_Header" style="text-align:center">3 Problem Analysis</h2>
<p>The current version 6.17 of Google Translate, people are only able to translate written based languages, either that be in the form of text entered or visually what the camera sees. Even more so, Google’s availability is quite limited when selecting languages to be translated using the camera. In the case of Google Translate, the user first opens the app, chooses the language to translate from, and then the language to be translated into. They then decide upon which form of translation they prefer, with most cases being available to choose from text, written, speech, or the camera. The camera is most useful when translating what a sign may mean when it’s hard to type a direct translation with letters and signs that the user may not be familiar with.  I know this idea is fairly possible as Google already has implemented a Chinese translation using the camera, which closely resembles the notion of communicating through concepts rather than direct articles such as “the” or “and”.  In conclusion, expanding on the interpretability of languages that do not involve written text, I will be focusing on the American Sign Language so the non signing community can interpret and understand the ASL language. </p>
<h2 id="Fourth_Point_Header" style="text-align:center">4 Background</h2>
<p>Hand signals were used as an early formation of sign language and have been around long before oral languages were ever thought to exist. From the earliest simple hand signals when hunting and gathering, to theoretical ways the ancient Native Americans communicated with other tribes, people still continue to use their hands when speaking today.  The first form of sign language communication was in 1760 when the first public school for the deaf opened in Paris, France. Many years later, a man by the name of Thomas H. Gallaudet was able to establish the first school in 1817 that taught what many consider the original version of American Sign Language. After some disputable years with many not believing in the validity of teaching sign language, ASL finally became an official language in 1960.
Google Translate was first launched in April of 2006, described as a statistical machine translation, taking one word at a time, translating it to English, and then statistically deciding the best option into the desired language. However, as of 2016, the app has switched to a neural machine translation operation, predicting sentences and phrases to translate, instead of individual words that may be out of context once they have been translated.
When communicating with the deaf community, oftentimes there are not many solutions available, having to rely on pen and paper to communicate.  The ability to read lips is an acquired ability that not all people possess or have a desire to bother with. According to Patrick Allan from lifehacker, it is best to also add a few gestures and facial expressions to further explain your message. Furthermore, it is a constant process to “always stay facing the other person(s), maintaining eye contact, and keeping your mouth visible”.  As for current mobile device ASL translators that are available on the market, most apps only deal with the one way translation of written or spoken English to ASL, portrayed by a virtual 3D avatar. This is a progressively positive step in the right direction for solving one way of communication and is better than none, however, the most needed form of communication is the other way around. Such as, when an encounter of a verbal/non verbal situation is in need of an urgent translation, as it may be the first time one is interacting with a deaf person, and can not read lips, it would be co-beneficial so that the level of communication is equalized and the situation is expeditiously resolved.
</p>
<h2 id="Fifth_Point_Header" style="text-align:center">5 Planning</h2>
<p>For the projected implementation of this project, the required planning can be implemented into two categories:  Before and After development.  With the main factor of this project relying on machine learning, it will be beneficial down the line to have a predetermined goal in accuracy and precision. With the idea of analysing single fingerspelled characters, the accuracy percentage will be compounded with each additional letter. With a presumed 10 percent error margin, already only five characters in will result in only a 50 percent accuracy as a whole. Within the initial testing stage, error correction can quickly be implemented to change how each letter or phrase is interpreted.  Once the app has been released, ideally it would be beneficial for both the user and myself, as the programmer, to see what parts of the app need improvement. This can be completed through voluntary community feedback which will naturally select out those problems that are more than minor inconveniences and need to be addressed. This can easily be implemented with a feedback section including a short survey of what the user likes and/or dislikes. Furthermore, as with any program involving machine learning, it is necessary for the algorithm to be constantly improved upon to provide a more accurate experience. An increase in real world applicable scenarios will help the algorithm understand what to look for instead of relying on just the testing scenarios that I created specifically for implementation. However this also raises the dilemma of data management, as the video needed to improve the learning process would include video footage that captures the majority of a person’s upper body. Therefore, the legal ramifications of recording without permission would have to be evaluated further and/or a disclaimer agreed to by the user would have to be implemented.
</p>
<h3 id="Fifth_Sub_Point_1" style="text-align:center">5.1 Software and Hardware Design</h3>
<p>A solution to this missing part of the market would be to create an add-on that could work with Google Translate that works with the smartphone’s built in camera to recognize sign language with machine learning recognition features. However since Google Translate is a copyrighted and protected app, I would be creating my own app user interface to represent what this feature could look like at its final stage of implementation. The hardware will be open to any smartphone that has a camera of decent quality and the processing power to handle image processing. For the software component of the project, the phone’s operating system will have to be Android for their greater flexibility in development and testing, and personally as it is where I have more experience in programming for web applications. However if the time restriction of only one semester was eliminated, the project idea would be expanded to include Apple devices and web development as needed. Internally, the project will consist of mainly Python. Once the user interface of the app has been created with Java in the Android Studio, the Python code will be separated into an executable file, and connected by the Runtime.exec() command. This is mainly due to its interpreter not being included on Android by default, and Python not being able to receive callbacks from the Java UI classes.</p>
<h3 id="Fifth_Sub_Point_2" style="text-align:center">5.2 Algorithm and Data Structure Selection</h3>
<p>Through the recent advancements and increasing popularity of programming languages such as Python, machine learning has made great strides to becoming commonplace in numerous fields. One such area that has seen numerous improvements is in the field of image analysis. The majority of software design for this project centers around this field, as real time translations using the camera with similar apps such as Google Translate become even more popular. The process of translating American Sign Language to English text consists of mainly two groups: the process of recording and understanding the ASL, and secondly, the process of taking that information and putting it into a coherent paragraph for the user to read. Breaking down the first group of recording and analysing, OpenCV will be used to cascade real time tests to detect and track the hands.  The motion detection is determined through absdiff() since video frames will be stored in numpy arrays. In relation to choosing to provide this service mainly on mobile devices, I will most likely convert the video to grayscale in order to reduce color complexity, since the analysis will have to be frame by frame using video_capture.read(). Blurring out the background around the person being recorded should help as well. With this process of hand identification, it will be fairly straightforward to take advantage with the keras and sklearn libraries to find an algorithm that works similar to the built-in face detection. The dataset will ideally already have another variable identifying what each of these hand signals represent, in which it will then go through the standard testing and training phases.  Onto the second group of analysis, the Stanford Parser will be used to work out the grammatical structure of sentences. Since Python will only be able to directly translate each signal to its direct word equivalent, the parser will take this as input and convert it into a grammatically correct and readable sentence. Furthermore, ASL is based on a subject-verb-object sentence structure, and thus since English is based on a similar format, the Parser will be able to correctly identify syntax trees and sentence length.
</p>
<h2 id="Sixth_Point_Header" style="text-align:center">6 User Interface Design</h2>
<p>Ideally, for users to be able to quickly transition to using this app, I would model it after other popular translator apps in both written text and ASL forms. First with the apps that translate written based languages, the main screen is usually split into simply the input and output sections. This helps eliminate complexity with only a binary choice to observe. With the apps that translate written or spoken word into an avatar that displays ASL, this also is usually the case, however the written text is usually near the bottom so that it is close to the keyboard and allows for the avatar to fill the main screen in the center. In both cases there is also a hamburger menu option in the top left or right corner that allows for access to secondary features such as profile and data privacy settings. Since my app relies on using the camera, it will open onto another screen when recording a person using ASL. The app will not immediately translate the hand signals during the recording phase as it will seem blocky and hard to understand without converting the grammar structure to written English. Plus this eliminates the unintended consequence of displaying a word that wasn’t properly translated yet. During this phase, only the hand detection will be shown on the screen with rectangles to notify the user if the video is in a good position for tracking. The results will be processed once the user presses 
“Done”, and then the translated text will be displayed. Furthermore, to prevent the possible error of running out of storage space, I intend to break up the recording into groups of two minutes, thus allowing for a part of the conversation to ensure being saved, and possibly to be translated in the background.  To help with the unknown variable of what situation this app may be used in, from casual to an emergency, the amount of time this particular algorithm takes to process the video would be an interesting factor needing further study. Once the text has been fully translated, it would be ideal to then be able to translate into other languages other than English, however this adds another layer of language translation loss and is not currently implemented.</p>
<h3 id="#Sixth_Sub_Point_1" style="text-align:center">6.1 Storyboard</h3>
<p>This storyboard gives a conceptual idea as to how the app will be used in the real world. Instead of scenarios such as podium speakers or conversations at home, the idea is to be used when a translator may not be readily available, such as most conversations that may happen out on the street. The characteristic of the conversation does not need to be so dire, but it helps emphasize why an immediate translation would be needed.</p>
<h3 id="Sixth_Sub_Point_2" style="text-align:center">6.2 UML Diagram</h3>
<p>This UML diagram covers an overview of how the app’s user interface will be visualized. Consisting of primarily two parts, there will be a login screen for multi-device synchronization, and the continuous loop of recording a conversation and translating the ASL into text of any language they choose, for now though only English is implemented. If this is their first time, they will be introduced to a tutorial to understand how the app works.</p>
<h3 id="Sixth_Sub_Point_3" style="text-align:center">6.3 User Interface Flowchart</h3>
<a href="https://xd.adobe.com/view/b56cb738-3cf7-4040-9363-24703b23b357-e03f/?fullscreen" target="_blank">User Interface</a><br><br>
<p>With the power of Adobe XD, a complete mockup of the user interface is possible, showing a more complete set of features that would be included with the app. The picture shows each individual screen correspondingly mapped to where its buttons will lead the user. The link above leads you to an interactive version of the user interface of the app if you would like to try it for yourself.</p>
<h2 id="Seventh_Point_Header" style="text-align:center">7 Verification and Validation</h2>
<p>In order to verify the accuracy of the app, scenarios will have to be tested that imitate a conversation being recorded for the phone to translate. Normal test cases will include the basics of the American Sign Language. However, much like how new words are added to the English dictionary every year, and how regional dialects may change the structure of a phrase of words, ASL is always evolving over time. Thus there may be some boundary cases where the words were not simply invented at the time when the machine was tested, and will most likely produce semi-incorrect results. Whereas illegal test cases will include any effort in trying to translate sign language that is from a different language origin.  For the purposes of this project, since the machine will only be tested against the American Sign Language, it will only be able to accept input that is also from the ASL. However with more time and processing power it could be feasible to create a choice enabling the machine to switch between different sign languages.
During the implementation of this application, the machine learning algorithm will need a training and testing set of ASL words and phrases to analyze for optimal hand recognition. To split up the entirety of the ASL language, I would theorize splitting the language into 5 categories of learning that I believe would represent a gradual increase in complexity.
</p>
<p>Level 1 begins with little hand movement and the analysis of only one hand signal at a time to understand the alphabet, spelling of other words, and numbers to signify quantity. Moving onto Level 2, these objects may require hand movement and sometimes more than one hand, but since they are at the core of the language, it would be best if the application has these down firmly. Next would be the fundamental building blocks of the language, with Level 3 being verbs. This is required at Level 3 because Level 4 consists of Verb-based Nouns, where the hand movements are very closely related to the basic forms of verbs. At this point a simple sentence could be made, but to make fluent sense, Level 5 would consist of all the finer details such as describing what tense the sentence is in, and for describing other objects and classifying people. This part of the algorithm would not handle knowing when to end and begin the next sentence however, where the results would then be handed to the Stanford Parser to decide.
</p>
<h2 id="Eigth_Point_Header" style="text-align:center">8 Prototyping</h2>
<h2 id="Ninth_Point_Header" style="text-align:center">9 Conclusion</h2>
<p>With the current lack of similar applications to provide a way of translating languages that are not spoken, I hope this project turns out to be an inspirational starting point for understanding the value of communicating with all people. This idea of an application can very easily be transferred into a wide variety of other non-spoken languages, enabling all cultures to be able to speak with their deaf community. Looking back when going into this project idea, I did not fully comprehend the complexity of American Sign Language and all of the words and phrases that it included. However, I believe it is still very much feasible as long as certain care is taken into testing and training the machine learning algorithms to be as accurate as possible so that the users will continue to trust its analysis within the app. Much like the Rosetta Stone was able to translate many archaic pictorial languages into more well-known languages, using this app will open up new ways of understanding, interpreting, and communicating with the American Sign Language community.
</p>
<h2 id="Tenth_Point_Header" style="text-align:center">10 References</h2>
<a href="https://www.pythonforengineers.com/image-and-video-processing-in-python/" target="_blank">https://www.pythonforengineers.com/image-and-video-processing-in-python/</a><br>
<a href="https://github.com/sahilkhoslaa/AudioToSignLanguageConverter" target="_blank">https://github.com/sahilkhoslaa/AudioToSignLanguageConverter</a><br>
<a href="https://nlp.stanford.edu/software/lex-parser.shtml" target="_blank">https://nlp.stanford.edu/software/lex-parser.shtml</a><br>
<a href="https://lifehacker.com/how-to-communicate-with-deaf-people-when-you-dont-know-1819512491" target="_blank">https://lifehacker.com/how-to-communicate-with-deaf-people-when-you-dont-know-1819512491</a><br>
</main>
</html>